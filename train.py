import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
from torch.utils.data import DataLoader, TensorDataset
from sklearn.model_selection import KFold
import shutil  # ç”¨äºå¤åˆ¶æ¨¡å‹

from config import Config


# ============================
# 0. é…ç½®ç¯å¢ƒ & æ•°æ®åŠ è½½
# ============================
Config.init()
Config.save_yaml()
device = torch.device(Config.DEVICE)
print(f"Device: {device}")

# åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆç¡®ä¿å­˜åœ¨ï¼‰
os.makedirs(Config.RESULT_SAVE_DIR, exist_ok=True)

# åŠ è½½è®­ç»ƒæ•°æ®
df = pd.read_csv(Config.TRAIN_CSV)
x = df.iloc[:, 0].values.astype(np.float32)
y = df.iloc[:, 1].values.astype(np.float32)

# å‡†å¤‡ KFold
kf = KFold(n_splits=Config.K_FOLDS, shuffle=True, random_state=Config.CV_SEED)

# å­˜å‚¨å„æŠ˜ç»“æœ
cv_val_losses = []
fold_models = []
fold_best_epochs = []  # âœ… æ–°å¢ï¼šè®°å½•å„ fold çš„æœ€ä½³ epoch


# ============================
# 1. K-Fold äº¤å‰éªŒè¯ä¸»å¾ªç¯
# ============================
print(f"\n{'='*60}")
print(f" ğŸ”„ Starting {Config.K_FOLDS}-Fold Cross-Validation")
print(f"{'='*60}")

for fold, (train_idx, val_idx) in enumerate(kf.split(x), 1):
    print(f"\n ğŸ” Fold {fold}/{Config.K_FOLDS}")

    # åˆ’åˆ†æ•°æ®
    x_train, y_train = x[train_idx], y[train_idx]
    x_val, y_val = x[val_idx], y[val_idx]

    # è½¬ Tensor
    X_train = torch.tensor(x_train.reshape(-1, 1), device=device)
    Y_train = torch.tensor(y_train.reshape(-1, 1), device=device)
    X_val = torch.tensor(x_val.reshape(-1, 1), device=device)
    Y_val = torch.tensor(y_val.reshape(-1, 1), device=device)

    train_loader = DataLoader(TensorDataset(X_train, Y_train), 
                              batch_size=Config.BATCH_SIZE, shuffle=True)
    val_loader = DataLoader(TensorDataset(X_val, Y_val), 
                            batch_size=Config.BATCH_SIZE, shuffle=False)

    # åˆå§‹åŒ–æ¨¡å‹ã€ä¼˜åŒ–å™¨ç­‰ï¼ˆæ¯æŠ˜é‡æ–°åˆå§‹åŒ–ï¼ï¼‰
    model = Config.MODEL_CLASS().to(device)
    criterion = Config.build_loss()
    optimizer = Config.build_optimizer(model)
    scheduler = Config.build_scheduler(optimizer)

    best_val_loss = float("inf")
    patience_count = 0
    train_loss_curve = []
    val_loss_curve = []
    best_epoch = 0

    # è®­ç»ƒå¾ªç¯
    for epoch in range(1, Config.EPOCHS + 1):
        # ----- è®­ç»ƒ -----
        model.train()
        train_losses = []
        for Xb, Yb in train_loader:
            pred = model(Xb)
            loss = criterion(pred, Yb)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            train_losses.append(loss.item())

        if scheduler is not None:
            scheduler.step()

        # ----- éªŒè¯ -----
        model.eval()
        val_losses = []
        with torch.no_grad():
            for Xe, Ye in val_loader:
                pred = model(Xe)
                loss = criterion(pred, Ye)
                val_losses.append(loss.item())

        train_loss = np.mean(train_losses)
        val_loss = np.mean(val_losses)
        train_loss_curve.append(train_loss)
        val_loss_curve.append(val_loss)

        # Early Stopping & Save Best
        if val_loss < best_val_loss:
            best_val_loss = val_loss
            patience_count = 0
            best_epoch = epoch  # âœ… è®°å½•å½“å‰æœ€ä½³ epoch

            # ä¿å­˜è¯¥ fold æœ€ä½³æ¨¡å‹ï¼ˆå¸¦ fold æ ‡è¯†ï¼‰
            model_path = os.path.join(
                Config.RESULT_SAVE_DIR, f"best_model_fold_{fold}.pth"
            )
            torch.save(model.state_dict(), model_path)
        else:
            patience_count += 1

        # æå‰åœæ­¢
        if patience_count >= Config.PATIENCE:
            print(f"  â¸ï¸ Early stopping at epoch {epoch} (best val loss: {best_val_loss:.6e})")
            break

        # æ—¥å¿—
        if epoch % 100 == 0 or epoch == Config.EPOCHS or patience_count >= Config.PATIENCE - 2:
            print(f"  Epoch {epoch:4d} | train={train_loss:.6e} | val={val_loss:.6e}")

    # ä¿å­˜è¯¥ fold çš„ loss æ›²çº¿å›¾
    plt.figure(figsize=(8, 4))
    epochs = range(1, len(train_loss_curve) + 1)
    plt.plot(epochs, train_loss_curve, label="Train Loss", alpha=0.8)
    plt.plot(epochs, val_loss_curve, label="Val Loss", alpha=0.8)
    plt.axvline(best_epoch, color='r', linestyle='--', linewidth=0.8, label=f'Best (ep {best_epoch})')
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.title(f"Fold {fold} Loss Curve (Best Val Loss: {best_val_loss:.2e})")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    plt.savefig(os.path.join(Config.RESULT_SAVE_DIR, f"loss_fold_{fold}.png"))
    plt.close()

    # è®°å½•ç»“æœ
    cv_val_losses.append(best_val_loss)
    fold_models.append(model_path)
    fold_best_epochs.append(best_epoch)  # âœ… ä¿å­˜ best epoch

    print(f"  âœ… Fold {fold} done. Best Val Loss: {best_val_loss:.6e} (epoch {best_epoch})")


# ============================
# 2. æ±‡æ€» CV ç»“æœ
# ============================
cv_val_losses = np.array(cv_val_losses)
mean_val_loss = cv_val_losses.mean()
std_val_loss = cv_val_losses.std()

print(f"\n{'='*60}")
print(f" ğŸ“Š {Config.K_FOLDS}-Fold Cross-Validation Results")
print(f"{'='*60}")
for i, vl in enumerate(cv_val_losses, 1):
    print(f"Fold {i:2d} Val Loss: {vl:.6e} (best epoch: {fold_best_epochs[i-1]})")
print(f"{'-'*60}")
print(f"Mean Val Loss: {mean_val_loss:.6e} Â± {std_val_loss:.6e}")
print(f"95% CI: [{mean_val_loss - 1.96*std_val_loss/np.sqrt(Config.K_FOLDS):.6e}, "
      f"{mean_val_loss + 1.96*std_val_loss/np.sqrt(Config.K_FOLDS):.6e}]")

# ä¿å­˜ç»“æœåˆ°æ–‡æœ¬
result_txt = os.path.join(Config.RESULT_SAVE_DIR, "cv_results.txt")
with open(result_txt, "w") as f:
    f.write(f"{Config.K_FOLDS}-Fold CV Results\n")
    f.write("="*60 + "\n")
    for i, (vl, ep) in enumerate(zip(cv_val_losses, fold_best_epochs), 1):
        f.write(f"Fold {i:2d}: Val Loss = {vl:.6e}, Best Epoch = {ep}\n")
    f.write("-"*60 + "\n")
    f.write(f"Mean Val Loss: {mean_val_loss:.6e}\n")
    f.write(f"Std:            {std_val_loss:.6e}\n")
    f.write(f"Avg Best Epoch: {int(np.mean(fold_best_epochs))}\n")

print(f"\nâœ… CV results saved to: {result_txt}")


# ============================
# 3. å¤åˆ¶ CV æœ€ä½³æ¨¡å‹ï¼ˆå¯é€‰å¤‡ä»½ï¼‰
# ============================
best_cv_fold_idx = int(np.argmin(cv_val_losses))  # 0-based
best_cv_model_path = fold_models[best_cv_fold_idx]
best_cv_epoch = fold_best_epochs[best_cv_fold_idx]

cv_best_save_path = os.path.join(Config.RESULT_SAVE_DIR, "best_model_cv.pth")
shutil.copy(best_cv_model_path, cv_best_save_path)
print(f"\nğŸ“¥ CV best model (Fold {best_cv_fold_idx+1}, epoch {best_cv_epoch}) "
      f"copied to: {cv_best_save_path}")


# ============================
# 4. å…¨é‡æ•°æ®é‡æ–°è®­ç»ƒï¼ˆRefit on Full Dataï¼‰
# ============================
print(f"\n{'='*60}")
print(" ğŸ” Retraining on Full Dataset")
print(f"{'='*60}")

# ç­–ç•¥ï¼šç”¨å„ fold æœ€ä½³ epoch çš„å¹³å‡å€¼ï¼ˆé¿å…è¿‡æ‹Ÿåˆï¼‰
retrain_epochs = int(np.round(np.mean(fold_best_epochs)))
print(f"ğŸ“ˆ Retrain Epochs = avg(best epochs) = {np.mean(fold_best_epochs):.1f} â†’ {retrain_epochs}")

# å‡†å¤‡å…¨é‡æ•°æ®
X_full = torch.tensor(x.reshape(-1, 1), device=device)
Y_full = torch.tensor(y.reshape(-1, 1), device=device)
full_loader = DataLoader(
    TensorDataset(X_full, Y_full),
    batch_size=Config.BATCH_SIZE,
    shuffle=True
)

# åˆå§‹åŒ–æœ€ç»ˆæ¨¡å‹
final_model = Config.MODEL_CLASS().to(device)
criterion = Config.build_loss()
optimizer = Config.build_optimizer(final_model)
scheduler = Config.build_scheduler(optimizer)

# å…¨é‡è®­ç»ƒ
print(f"ğŸš€ Training on full data ({len(x)} samples) for {retrain_epochs} epochs...")
train_loss_curve_full = []

for epoch in range(1, retrain_epochs + 1):
    final_model.train()
    batch_losses = []
    
    for Xb, Yb in full_loader:
        pred = final_model(Xb)
        loss = criterion(pred, Yb)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        batch_losses.append(loss.item())
    
    if scheduler is not None:
        scheduler.step()
    
    epoch_loss = np.mean(batch_losses)
    train_loss_curve_full.append(epoch_loss)
    
    if epoch % 50 == 0 or epoch == retrain_epochs:
        print(f"  Epoch {epoch:4d}/{retrain_epochs} | train_loss = {epoch_loss:.6e}")

# ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°æ ‡å‡†è·¯å¾„ï¼ˆConfig.MODEL_SAVE_PATHï¼‰
torch.save(final_model.state_dict(), Config.MODEL_SAVE_PATH)
print(f"\nâœ… Final model (full-data retrain) saved to: {Config.MODEL_SAVE_PATH}")

# ä¿å­˜å…¨é‡è®­ç»ƒ loss æ›²çº¿
plt.figure(figsize=(8, 4))
epochs = range(1, len(train_loss_curve_full) + 1)
plt.plot(epochs, train_loss_curve_full, label="Full Train Loss", color='purple', linewidth=1.5)
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.title(f"Full-Data Training (Epochs={retrain_epochs})")
plt.grid(True)
plt.legend()
plt.tight_layout()
full_loss_fig = os.path.join(Config.RESULT_SAVE_DIR, "loss_full_train.png")
plt.savefig(full_loss_fig)
plt.close()
print(f"ğŸ“Š Full-train loss curve saved to: {full_loss_fig}")


# ============================
# 5. æ€»ç»“
# ============================
print(f"\n{'='*60}")
print(" âœ… Training Pipeline Completed!")
print(f"{'='*60}")
print(f"â€¢ CV Mean Val Loss: {mean_val_loss:.6e} Â± {std_val_loss:.6e}")
print(f"â€¢ Final Model (full retrain): {Config.MODEL_SAVE_PATH}")
print(f"â€¢ CV Best Model (backup):      {cv_best_save_path}")
print(f"â€¢ Results Directory:           {Config.RESULT_SAVE_DIR}")
print(f"{'='*60}")





